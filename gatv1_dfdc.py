# -*- coding: utf-8 -*-
"""GATv1_DFDC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V9-SdjYvzyreN5-VL_l_Q6XWRk189lp1

# GATv1
"""

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'dfdcdfdc:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4256754%2F7332880%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241010%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241010T201418Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4f6ddfa95db962f8276ab291f1e389bfed057241e99c2bd8132034da002d6b3f7474c5310ddc2f3e5134d5ff82600a32f399d7709c11879e5145bd0fdb43b9f6e4d277dd7ac90721d6a443aed8588364fe2552de6ac511786816e8513d01da384be1a5b9b4c129074df2f9f26065384a2716bf332685d2d05050fa8391d4bc169de1a1ca1fb38c293b435d43809a5987786a19e132836fb9e9f5fd599cd74a10aaccd0266bcca62b1811387d3f46df7d69c745b2e53e770d6f446d5c294084108411503dbd724d3655abd26c82e4a0e07fb988bcebb2d6b25298eb60b32597b171e7208902bb0e2b5fbec4ae983fea6b8a46cac7bd052b53ebdbd4a37573e5ff'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

!pip install torch_geometric

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import models, transforms
from PIL import Image
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np

from torch_geometric.nn import GATConv
from torch_geometric.data import Data

from tqdm import tqdm

class ASDataset(Dataset):
    def __init__(self, client_file: str, imposter_file: str, transforms=None):
        with open(client_file, "r") as f:
            client_files = f.read().splitlines()
        with open(imposter_file, "r") as f:
            imposter_files = f.read().splitlines()
        self.labels = torch.cat((torch.ones(len(client_files)), torch.zeros(len(imposter_files))))
        self.imgs = client_files + imposter_files
        self.transforms = transforms

    def __len__(self):
        return len(self.imgs)

    def __getitem__(self, idx):
        img_name = self.imgs[idx]
        img = Image.open(img_name).convert('RGB')
        label = self.labels[idx]
        if self.transforms:
            img = self.transforms(img)
        return img, label

preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

train_dataset = ASDataset(
    client_file="/kaggle/input/dfdcdfdc/TEST_CLIENT.txt", imposter_file="/kaggle/input/dfdcdfdc/TEST_IMPOSTER.txt",
    transforms=preprocess
)
val_dataset = ASDataset(
    client_file="/kaggle/input/dfdcdfdc/TRAIN_CLIENT.txt", imposter_file="/kaggle/input/dfdcdfdc/TRAIN_IMPOSTER.txt",
    transforms=preprocess
)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)

class ResNetGAT(nn.Module):
    def __init__(self, num_classes=1):
        super(ResNetGAT, self).__init__()
        resnet = models.resnet50(pretrained=True)
        self.backbone = nn.Sequential(*list(resnet.children())[:-2])
        self.gat_layer = GATConv(in_channels=2048, out_channels=512, heads=9, concat=True)
        self.fc = nn.Linear(512*9, num_classes)

    def forward(self, x):
        batch_size = x.size(0)
        x = self.backbone(x)
        x = x.view(batch_size, 2048, -1)
        x = x.permute(0, 2, 1)

        all_outputs = []
        attentions = []
        for i in range(batch_size):
            node_features = x[i]
            edge_index = create_grid_edge_index(7, 7, device=x.device)
            node_features, attn = self.gat_layer(node_features, edge_index, return_attention_weights=True)
            x_pool = node_features.mean(dim=0)
            all_outputs.append(x_pool)
            attentions.append(attn)

        x = torch.stack(all_outputs)
        x = self.fc(x)
        return x, attentions


def create_grid_edge_index(height, width, device):
    edge_index = []
    for i in range(height):
        for j in range(width):
            idx = i * width + j
            if j < width - 1:
                right_idx = i * width + (j + 1)
                edge_index.append([idx, right_idx])
                edge_index.append([right_idx, idx])
            if i < height - 1:
                bottom_idx = (i + 1) * width + j
                edge_index.append([idx, bottom_idx])
                edge_index.append([bottom_idx, idx])
    edge_index = torch.tensor(edge_index, dtype=torch.long, device=device).t()
    return edge_index

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = ResNetGAT().to(device)
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=3e-4)

num_epochs = 15
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for imgs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        imgs = imgs.to(device)
        labels = labels.unsqueeze(1).to(device)
        optimizer.zero_grad()
        outputs, _ = model(imgs)
        loss = criterion(outputs, labels.float())
        loss.backward()
        optimizer.step()
        running_loss += loss.item() * imgs.size(0)
    epoch_loss = running_loss / len(train_loader.dataset)
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')

"""# Examples of Interpretation"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from torchvision import transforms, models
from PIL import Image
from torch_geometric.nn import GATv2Conv

image_paths = [
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/efwfxwwlbw.mp4_face_1.jpg', #REAL
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/eivxffliio.mp4_face_1.jpg', #FAKE
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/dwediigjit.mp4_face_1.jpg', #FAKE
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/dsndhujjjb.mp4_face_1.jpg', #FAKE

# --------

    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/dkuayagnmc.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/ahbweevwpv.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/aagfhgtpmv.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/atxvxouljq.mp4_face_1.jpg',

# --------

    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/caifxvsozs.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/bpwzipqtxf.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/avibnnhwhp.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/aapnvogymq.mp4_face_1.jpg'
]

preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def process_image(image_path):
    img = Image.open(image_path).convert('RGB')
    img_preprocessed = preprocess(img)
    img_preprocessed = img_preprocessed.unsqueeze(0).to(device)

    with torch.no_grad():
        outputs, attentions = model(img_preprocessed)

    original_img = np.array(img.resize((224, 224))) / 255.0

    edge_index, alpha = attentions[0]
    edge_index = edge_index.cpu().numpy()
    alpha = alpha.cpu().detach().numpy()

    alpha_mean = alpha.mean(axis=1)

    num_nodes = 7 * 7
    node_attention = np.zeros(num_nodes)
    for i in range(alpha_mean.shape[0]):
        src = edge_index[0, i]
        node_attention[src] += alpha_mean[i]

    node_attention = node_attention.reshape(7, 7)
    node_attention = (node_attention - node_attention.min()) / (node_attention.max() - node_attention.min())

    attention_map = np.array(Image.fromarray(node_attention).resize((224, 224), resample=Image.BILINEAR))

    return original_img, attention_map

plt.figure(figsize=(20, len(image_paths) * 5))

for i, image_path in enumerate(image_paths):
    original_img, attention_map = process_image(image_path)

    plt.subplot(len(image_paths), 2, 2 * i + 1)
    plt.imshow(original_img)
    plt.title(f'Оригинальное изображение {i+1}')
    plt.axis('off')

    plt.subplot(len(image_paths), 2, 2 * i + 2)
    plt.imshow(original_img)
    plt.imshow(attention_map, cmap='jet', alpha=0.5)
    plt.title(f'Карта внимания {i+1}')
    plt.axis('off')

plt.show()

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from torchvision import transforms, models
from PIL import Image
from torch_geometric.nn import GATv2Conv

image_paths = [
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/tjywwgftmv.mp4_face_94.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/njzshtfmcw.mp4_face_79.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/gfdjzwnpyp.mp4_face_79.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/eivxffliio.mp4_face_74.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/eudeqjhdfd.mp4_face_87.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/nymodlmxni.mp4_face_79.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/efwfxwwlbw.mp4_face_79.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/ecujsjhscd.mp4_face_79.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/eggbjzxnmg.mp4_face_70.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/tgawasvbbr.mp4_face_52.jpg'
]

preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def process_image(image_path):
    img = Image.open(image_path).convert('RGB')
    img_preprocessed = preprocess(img)
    img_preprocessed = img_preprocessed.unsqueeze(0).to(device)

    with torch.no_grad():
        outputs, attentions = model(img_preprocessed)

    original_img = np.array(img.resize((224, 224))) / 255.0

    edge_index, alpha = attentions[0]
    edge_index = edge_index.cpu().numpy()
    alpha = alpha.cpu().detach().numpy()

    alpha_mean = alpha.mean(axis=1)

    num_nodes = 7 * 7
    node_attention = np.zeros(num_nodes)
    for i in range(alpha_mean.shape[0]):
        src = edge_index[0, i]
        node_attention[src] += alpha_mean[i]

    node_attention = node_attention.reshape(7, 7)
    node_attention = (node_attention - node_attention.min()) / (node_attention.max() - node_attention.min())

    attention_map = np.array(Image.fromarray(node_attention).resize((224, 224), resample=Image.BILINEAR))

    return original_img, attention_map

plt.figure(figsize=(20, len(image_paths) * 5))

for i, image_path in enumerate(image_paths):
    original_img, attention_map = process_image(image_path)

    plt.subplot(len(image_paths), 2, 2 * i + 1)
    plt.imshow(original_img)
    plt.title(f'Оригинальное изображение {i+1}')
    plt.axis('off')

    plt.subplot(len(image_paths), 2, 2 * i + 2)
    plt.imshow(original_img)
    plt.imshow(attention_map, cmap='jet', alpha=0.5)
    plt.title(f'Карта внимания {i+1}')
    plt.axis('off')

plt.show()

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from torchvision import transforms, models
from PIL import Image
from torch_geometric.nn import GATv2Conv

image_paths = [
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/ehfiekigla.mp4_face_99.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/eiwopxzjfn.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/ekhacizpah.mp4_face_9.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/dvumqqhoac.mp4_face_15.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/dxuplhwvig.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/dzwkmcwkwl.mp4_face_2.jpg',


    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/xcruhaccxc.mp4_face_82.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/xhtppuyqdr.mp4_face_78.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/xmkwsnuzyq.mp4_face_19.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/uhakqelqri.mp4_face_7.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/uoccaiathd.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/uqvxjfpwdo.mp4_face_24.jpg'
]

preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def process_image(image_path):
    img = Image.open(image_path).convert('RGB')
    img_preprocessed = preprocess(img)
    img_preprocessed = img_preprocessed.unsqueeze(0).to(device)

    with torch.no_grad():
        outputs, attentions = model(img_preprocessed)

    original_img = np.array(img.resize((224, 224))) / 255.0

    edge_index, alpha = attentions[0]
    edge_index = edge_index.cpu().numpy()
    alpha = alpha.cpu().detach().numpy()

    alpha_mean = alpha.mean(axis=1)

    num_nodes = 7 * 7
    node_attention = np.zeros(num_nodes)
    for i in range(alpha_mean.shape[0]):
        src = edge_index[0, i]
        node_attention[src] += alpha_mean[i]

    node_attention = node_attention.reshape(7, 7)
    node_attention = (node_attention - node_attention.min()) / (node_attention.max() - node_attention.min())

    attention_map = np.array(Image.fromarray(node_attention).resize((224, 224), resample=Image.BILINEAR))

    return original_img, attention_map

plt.figure(figsize=(20, len(image_paths) * 5))

for i, image_path in enumerate(image_paths):
    original_img, attention_map = process_image(image_path)

    plt.subplot(len(image_paths), 2, 2 * i + 1)
    plt.imshow(original_img)
    plt.title(f'Оригинальное изображение {i+1}')
    plt.axis('off')

    plt.subplot(len(image_paths), 2, 2 * i + 2)
    plt.imshow(original_img)
    plt.imshow(attention_map, cmap='jet', alpha=0.5)
    plt.title(f'Карта внимания {i+1}')
    plt.axis('off')

plt.show()

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from torchvision import transforms, models
from PIL import Image
from torch_geometric.nn import GATv2Conv

image_paths = [
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/ebchwmwayp.mp4_face_23.jpg', #fake
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/abarnvbtwb.mp4_face_44.jpg'  #real
]

preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def process_image(image_path):
    img = Image.open(image_path).convert('RGB')
    img_preprocessed = preprocess(img)
    img_preprocessed = img_preprocessed.unsqueeze(0).to(device)

    with torch.no_grad():
        outputs, attentions = model(img_preprocessed)

    original_img = np.array(img.resize((224, 224))) / 255.0

    edge_index, alpha = attentions[0]
    edge_index = edge_index.cpu().numpy()
    alpha = alpha.cpu().detach().numpy()

    alpha_mean = alpha.mean(axis=1)

    num_nodes = 7 * 7
    node_attention = np.zeros(num_nodes)
    for i in range(alpha_mean.shape[0]):
        src = edge_index[0, i]
        node_attention[src] += alpha_mean[i]

    node_attention = node_attention.reshape(7, 7)
    node_attention = (node_attention - node_attention.min()) / (node_attention.max() - node_attention.min())

    attention_map = np.array(Image.fromarray(node_attention).resize((224, 224), resample=Image.BILINEAR))

    return original_img, attention_map

plt.figure(figsize=(20, len(image_paths) * 5))

for i, image_path in enumerate(image_paths):
    original_img, attention_map = process_image(image_path)

    plt.subplot(len(image_paths), 2, 2 * i + 1)
    plt.imshow(original_img)
    plt.title(f'Оригинальное изображение {i+1}')
    plt.axis('off')

    plt.subplot(len(image_paths), 2, 2 * i + 2)
    plt.imshow(original_img)
    plt.imshow(attention_map, cmap='jet', alpha=0.5)
    plt.title(f'Карта внимания {i+1}')
    plt.axis('off')

plt.show()

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from torchvision import transforms, models
from PIL import Image
from torch_geometric.nn import GATv2Conv

image_paths = [
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/eivxffliio.mp4_face_100.jpg', #fake
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/xdezcezszc.mp4_face_100.jpg'  #real
]

preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def process_image(image_path):
    img = Image.open(image_path).convert('RGB')
    img_preprocessed = preprocess(img)
    img_preprocessed = img_preprocessed.unsqueeze(0).to(device)

    with torch.no_grad():
        outputs, attentions = model(img_preprocessed)

    original_img = np.array(img.resize((224, 224))) / 255.0

    edge_index, alpha = attentions[0]
    edge_index = edge_index.cpu().numpy()
    alpha = alpha.cpu().detach().numpy()

    alpha_mean = alpha.mean(axis=1)

    num_nodes = 7 * 7
    node_attention = np.zeros(num_nodes)
    for i in range(alpha_mean.shape[0]):
        src = edge_index[0, i]
        node_attention[src] += alpha_mean[i]

    node_attention = node_attention.reshape(7, 7)
    node_attention = (node_attention - node_attention.min()) / (node_attention.max() - node_attention.min())

    attention_map = np.array(Image.fromarray(node_attention).resize((224, 224), resample=Image.BILINEAR))

    return original_img, attention_map

plt.figure(figsize=(20, len(image_paths) * 5))

for i, image_path in enumerate(image_paths):
    original_img, attention_map = process_image(image_path)

    plt.subplot(len(image_paths), 2, 2 * i + 1)
    plt.imshow(original_img)
    plt.title(f'Оригинальное изображение {i+1}')
    plt.axis('off')

    plt.subplot(len(image_paths), 2, 2 * i + 2)
    plt.imshow(original_img)
    plt.imshow(attention_map, cmap='jet', alpha=0.5)
    plt.title(f'Карта внимания {i+1}')
    plt.axis('off')

plt.show()

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import numpy as np
from torchvision import transforms, models
from PIL import Image
from torch_geometric.nn import GATv2Conv

image_paths = [
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/acifjvzvpm.mp4_face_29.jpg', #fake
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/atkdltyyen.mp4_face_107.jpg'  #real
]

preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def process_image(image_path):
    img = Image.open(image_path).convert('RGB')
    img_preprocessed = preprocess(img)
    img_preprocessed = img_preprocessed.unsqueeze(0).to(device)

    with torch.no_grad():
        outputs, attentions = model(img_preprocessed)

    original_img = np.array(img.resize((224, 224))) / 255.0

    edge_index, alpha = attentions[0]
    edge_index = edge_index.cpu().numpy()
    alpha = alpha.cpu().detach().numpy()

    alpha_mean = alpha.mean(axis=1)

    num_nodes = 7 * 7
    node_attention = np.zeros(num_nodes)
    for i in range(alpha_mean.shape[0]):
        src = edge_index[0, i]
        node_attention[src] += alpha_mean[i]

    node_attention = node_attention.reshape(7, 7)
    node_attention = (node_attention - node_attention.min()) / (node_attention.max() - node_attention.min())

    attention_map = np.array(Image.fromarray(node_attention).resize((224, 224), resample=Image.BILINEAR))

    return original_img, attention_map

plt.figure(figsize=(20, len(image_paths) * 5))

for i, image_path in enumerate(image_paths):
    original_img, attention_map = process_image(image_path)

    plt.subplot(len(image_paths), 2, 2 * i + 1)
    plt.imshow(original_img)
    plt.title(f'Оригинальное изображение {i+1}')
    plt.axis('off')

    plt.subplot(len(image_paths), 2, 2 * i + 2)
    plt.imshow(original_img)
    plt.imshow(attention_map, cmap='jet', alpha=0.5)
    plt.title(f'Карта внимания {i+1}')
    plt.axis('off')

plt.show()

"""# Metrics"""

torch.save(model.state_dict(), 'model_weights.pth')

torch.save(model, 'model_full.pth')

import torch
from sklearn.metrics import f1_score, roc_auc_score, confusion_matrix
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns

def calculate_metrics(model, loader, device):
    model.eval()
    all_labels = []
    all_predictions = []
    all_probs = []

    with torch.no_grad():
        for imgs, labels in tqdm(loader, desc="Calculating Metrics"):
            imgs = imgs.to(device)
            labels = labels.unsqueeze(1).to(device)

            outputs, _ = model(imgs)

            probabilities = torch.sigmoid(outputs)
            predicted = (probabilities > 0.5).float()

            all_labels.extend(labels.cpu().numpy())
            all_predictions.extend(predicted.cpu().numpy())
            all_probs.extend(probabilities.cpu().numpy())

    f1 = f1_score(all_labels, all_predictions, average='binary')

    correct = (torch.tensor(all_predictions) == torch.tensor(all_labels)).sum().item()
    accuracy = correct / len(all_labels)

    roc_auc = roc_auc_score(all_labels, all_probs)

    cm = confusion_matrix(all_labels, all_predictions)
    return f1, accuracy, roc_auc, cm

def plot_confusion_matrix(cm):
    plt.figure(figsize=(6, 4))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

model = ResNetGAT().to(device)
model.load_state_dict(torch.load('model_weights.pth'))

f1, accuracy, roc_auc, cm = calculate_metrics(model, val_loader, device)

print(f'F1 Score: {f1:.4f}')
print(f'Accuracy: {accuracy * 100:.2f}%')
print(f'ROC AUC: {roc_auc:.4f}')

plot_confusion_matrix(cm)

"""# GRAD CAM, LIME"""

pip install shap

pip install torchcam

pip install lime

import torch
from torchcam.methods import GradCAM
from torchvision.transforms import transforms
from torchcam.utils import overlay_mask
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

model = ResNetGAT().to(device)
model.eval()

cam_extractor = GradCAM(model, target_layer='backbone.7')

def visualize_cam(image_path, model, cam_extractor, device):
    img = Image.open(image_path).convert('RGB')
    img_tensor = preprocess(img).unsqueeze(0).to(device)

    img_tensor.requires_grad = True
    scores, _ = model(img_tensor)

    activation_map = cam_extractor(scores.squeeze(0).argmax().item(), scores)
    activation_map = activation_map[0].squeeze(0).cpu().numpy()

    original_img = np.array(img.resize((224, 224)))

    result = overlay_mask(
        Image.fromarray(original_img),
        Image.fromarray(activation_map, mode='F'),
        alpha=0.5
    )

    plt.imshow(result)
    plt.axis('off')
    plt.show()

for image_path in image_paths:
    visualize_cam(image_path, model, cam_extractor, device)

image_paths = [
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/efwfxwwlbw.mp4_face_1.jpg', #REAL
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/eivxffliio.mp4_face_1.jpg', #FAKE
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/dwediigjit.mp4_face_1.jpg', #FAKE
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/dsndhujjjb.mp4_face_1.jpg', #FAKE

# --------

    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/dkuayagnmc.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/ahbweevwpv.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/aagfhgtpmv.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/atxvxouljq.mp4_face_1.jpg',

# --------

    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/caifxvsozs.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/bpwzipqtxf.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/avibnnhwhp.mp4_face_1.jpg',
    '/kaggle/input/dfdcdfdc/DFDCDFDC/DFDCDFDC/aapnvogymq.mp4_face_1.jpg'
]

import torch
from torchcam.methods import GradCAM
from torchvision.transforms import transforms
from torchcam.utils import overlay_mask
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np

preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

model = ResNetGAT().to(device)
model.eval()

cam_extractor = GradCAM(model, target_layer='backbone.7')

def visualize_cam(image_path, model, cam_extractor, device):
    img = Image.open(image_path).convert('RGB')
    img_tensor = preprocess(img).unsqueeze(0).to(device)

    img_tensor.requires_grad = True

    scores, _ = model(img_tensor)

    activation_map = cam_extractor(scores.squeeze(0).argmax().item(), scores)

    activation_map = activation_map[0].squeeze(0).cpu().numpy()

    original_img = np.array(img.resize((224, 224)))

    result = overlay_mask(
        Image.fromarray(original_img),
        Image.fromarray(activation_map, mode='F'),
        alpha=0.5
    )

    plt.imshow(result)
    plt.axis('off')
    plt.show()

for image_path in image_paths:
    visualize_cam(image_path, model, cam_extractor, device)

class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.model.eval()
        self.gradients = None
        self.activations = None
        self.target_layer = target_layer

        self.target_layer.register_forward_hook(self.save_activation)
        self.target_layer.register_backward_hook(self.save_gradient)

    def save_activation(self, module, input, output):
        self.activations = output.detach()

    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0].detach()

    def __call__(self, x, index=None):
        x = x.to(next(self.model.parameters()).device)
        self.model.zero_grad()
        outputs, _ = self.model(x)
        if index is None:
            index = outputs.argmax(dim=1).item()

        target = outputs[:, index]
        target.backward()

        gradients = self.gradients
        activations = self.activations

        weights = gradients.mean(dim=(2, 3), keepdim=True)
        cam = (weights * activations).sum(dim=1, keepdim=True)
        cam = F.relu(cam)
        cam = F.interpolate(cam, size=(224, 224), mode='bilinear', align_corners=False)
        cam = cam.squeeze().cpu().numpy()

        cam -= cam.min()
        cam /= cam.max()
        return cam

target_layer = model.backbone[-1]
grad_cam = GradCAM(model, target_layer)

for image_path in image_paths:
    img = Image.open(image_path).convert('RGB')
    img_tensor = preprocess(img).unsqueeze(0).to(device)

    cam = grad_cam(img_tensor)

    plt.imshow(np.array(img.resize((224, 224))))
    plt.imshow(cam, cmap='jet', alpha=0.5)
    plt.title(f'Grad-CAM: {image_path}')
    plt.axis('off')
    plt.show()

from lime import lime_image
from skimage.segmentation import mark_boundaries

def batch_predict(images):
    images = [preprocess(Image.fromarray(img)).to(device) for img in images]
    images = torch.stack(images)
    outputs, _ = model(images)
    probs = torch.sigmoid(outputs).detach().cpu().numpy()
    return np.concatenate((1 - probs, probs), axis=1)

explainer = lime_image.LimeImageExplainer()

for image_path in image_paths:
    img = Image.open(image_path).convert('RGB')
    img_np = np.array(img.resize((224, 224)))

    explanation = explainer.explain_instance(
        img_np,
        batch_predict,
        top_labels=1,
        hide_color=0,
        num_samples=1000
    )

    temp, mask = explanation.get_image_and_mask(
        explanation.top_labels[0],
        positive_only=True,
        num_features=5,
        hide_rest=False
    )
    img_boundry = mark_boundaries(temp / 255.0, mask)
    plt.imshow(img_boundry)
    plt.title(f'LIME: {image_path}')
    plt.axis('off')
    plt.show()

class ResNetGAT(nn.Module):
    def __init__(self, num_classes=1):
        super(ResNetGAT, self).__init__()
        resnet = models.resnet50(pretrained=True)
        self.backbone = nn.Sequential(*list(resnet.children())[:-2])
        self.gat_layer = GATConv(in_channels=2048, out_channels=512, heads=9, concat=True)

        self.fc = nn.Linear(512*9, num_classes)

    def forward(self, x):
        batch_size = x.size(0)
        x = self.backbone(x)
        x = x.view(batch_size, 2048, -1)
        x = x.permute(0, 2, 1)

        all_outputs = []
        attentions = []
        for i in range(batch_size):
            node_features = x[i]
            edge_index = create_grid_edge_index(7, 7, device=x.device)
            node_features, attn = self.gat_layer(node_features, edge_index, return_attention_weights=True)
            x_pool = node_features.mean(dim=0)
            all_outputs.append(x_pool)
            attentions.append(attn)

        x = torch.stack(all_outputs)
        x = self.fc(x)

        if return_attention:
            return x, attentions
        else:
            return x, None

pip install captum

class ResNetGAT(nn.Module):
    def __init__(self, num_classes=1):
        super(ResNetGAT, self).__init__()
        resnet = models.resnet50(pretrained=True)
        self.backbone = nn.Sequential(*list(resnet.children())[:-2])
        self.gat_layer = GATConv(in_channels=2048, out_channels=512, heads=9, concat=True)

        self.fc = nn.Linear(512*9, num_classes)

    def forward(self, x):
        batch_size = x.size(0)
        x = self.backbone(x)
        x = x.view(batch_size, 2048, -1)
        x = x.permute(0, 2, 1)

        all_outputs = []
        attentions = []
        for i in range(batch_size):
            node_features = x[i]
            edge_index = create_grid_edge_index(7, 7, device=x.device)
            node_features, attn = self.gat_layer(node_features, edge_index, return_attention_weights=True)
            x_pool = node_features.mean(dim=0)
            all_outputs.append(x_pool)
            attentions.append(attn)

        x = torch.stack(all_outputs)
        x = self.fc(x)
        return x

import torch
import torch.nn as nn

class WrappedModel(nn.Module):
    def __init__(self, model):
        super(WrappedModel, self).__init__()
        self.model = model

    def forward(self, x):
        outputs, _ = self.model(x, return_attention=False)
        return outputs

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = ResNetGAT().to(device)
model.eval()

wrapped_model = WrappedModel(model).to(device)
wrapped_model.eval()

wrapped_model = ResNetGAT().to(device)
wrapped_model.eval()

from captum.attr import Saliency

saliency = Saliency(wrapped_model)

for image_path in image_paths:
    img = Image.open(image_path).convert('RGB')
    img_tensor = preprocess(img).unsqueeze(0).to(device)
    img_tensor.requires_grad = True

    attributions = saliency.attribute(img_tensor, target=0)

    attributions = attributions.squeeze().cpu().detach().numpy()
    attributions = np.abs(attributions).sum(axis=0)

    attributions = (attributions - attributions.min()) / (attributions.max() - attributions.min())

    plt.imshow(np.array(img.resize((224, 224))), alpha=0.8)
    plt.imshow(attributions, cmap='hot', alpha=0.5)
    plt.title(f'Saliency Map: {image_path}')
    plt.axis('off')
    plt.show()

from captum.attr import IntegratedGradients

ig = IntegratedGradients(wrapped_model)

for image_path in image_paths:
    img = Image.open(image_path).convert('RGB')
    img_tensor = preprocess(img).unsqueeze(0).to(device)

    attributions, delta = ig.attribute(img_tensor, target=0, return_convergence_delta=True)

    attributions = attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0)
    attributions = np.mean(attributions, axis=2)

    attributions = (attributions - attributions.min()) / (attributions.max() - attributions.min())

    plt.imshow(np.array(img.resize((224, 224))), alpha=0.8)
    plt.imshow(attributions, cmap='hot', alpha=0.5)
    plt.title(f'Integrated Gradients: {image_path}')
    plt.axis('off')
    plt.show()

from captum.attr import GradientShap

gradient_shap = GradientShap(wrapped_model)

background = torch.cat([img_tensor * 0, img_tensor * 1])

for image_path in image_paths:
    img = Image.open(image_path).convert('RGB')
    img_tensor = preprocess(img).unsqueeze(0).to(device)
    img_tensor.requires_grad = True

    attributions = gradient_shap.attribute(
        img_tensor,
        baselines=background,
        target=0,
        n_samples=50
    )

    attributions = attributions.squeeze().cpu().detach().numpy().transpose(1, 2, 0)
    attributions = np.mean(attributions, axis=2)

    attributions = (attributions - attributions.min()) / (attributions.max() - attributions.min())

    plt.imshow(np.array(img.resize((224, 224))), alpha=0.8)
    plt.imshow(attributions, cmap='hot', alpha=0.5)
    plt.title(f'Gradient SHAP: {image_path}')
    plt.axis('off')
    plt.show()